# Exploring LLMs

Explorando as capacidades dos Modelos de Linguagem (LLMs) com a biblioteca Transformers da Hugging Face.

## üìö Checkpoint 04

**Autores:**  
- Jo√£o Victor Santos Alves (RM99634)  
- Luiza Barros Hubert (RM97905)  
- Suellen Guedes Rufino (RM97687)  

## üóÇÔ∏è Notebooks

1. [EntendendoLLMs.ipynb](./EntendendoLLMs.ipynb)
2. [Tokens,DevicesEMem√≥riaDeConversa.ipynb](./Tokens,DevicesEMem√≥riaDeConversa.ipynb)
3. [FiltrandoJailbreak.ipynb](./FiltrandoJailbreak.ipynb)

---

## üìñ Descri√ß√£o do Projeto

Este projeto visa desenvolver um sistema de detec√ß√£o de prompts de jailbreak utilizando modelos de linguagem (LLMs) ajustados. O sistema combina dois modelos: um classificador de prompts e um gerador de texto.

### üîç O que √© Fine-Tuning?

**Fine-tuning** √© o processo de ajustar um modelo de aprendizado de m√°quina pr√©-treinado em uma nova tarefa, utilizando um conjunto de dados espec√≠fico. Isso permite que o modelo aprenda a realizar tarefas especializadas, melhorando seu desempenho em situa√ß√µes espec√≠ficas.

### üö® O que √© Jailbreak?

**Jailbreak** refere-se a t√©cnicas utilizadas para contornar as limita√ß√µes de seguran√ßa de um modelo de linguagem, possibilitando a gera√ß√£o de conte√∫dos indesejados ou maliciosos. Prompts de jailbreak s√£o comandos projetados para manipular o comportamento do modelo, levando-o a gerar respostas inadequadas.

## ‚öôÔ∏è Implementa√ß√£o

Neste notebook, utilizamos:

- **Classificador de Jailbreak:** dispon√≠vel em [Hugging Face](https://huggingface.co/jackhhao/jailbreak-classifier)
- **Modelo de Gera√ß√£o de Texto:** [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)

### üõ†Ô∏è Funcionalidade

A fun√ß√£o principal do sistema √© a seguinte:

```python

def processa_prompt(modelo, prompt):
  print("="*80)

  # Codificando o prompt em tokens
  tokens = modelo.tokenizer.encode(prompt, return_tensors='pt')
  print(f"- prompt original: {prompt}")
  print(f"- tokens: {tokens}")
  print(f"- # de tokens: {len(tokens)}")

  # Decodificando os tokens
  decoded = modelo.tokenizer.convert_ids_to_tokens(tokens[0])
  print(f"- tokens decodificados: {decoded}")

  first_tokens = modelo.tokenizer.convert_ids_to_tokens(range(5))
  print(f"- primeiros tokens: {first_tokens}")

  # Processando o prompt com o modelo
  # O n√≠vel de "criatividade" do modelo √© alterado para 0.7 atrav√©s do par√¢metro "temperature"
  # √â necess√°rio setar "do_sample"=True para controlar a temperatura
  output = modelo(prompt, do_sample=True, temperature=0.7)
  print(f"- output do modelo: {output}")
  print(f"- texto gerado efetivamente: {output[0]['generated_text']}")

  print("="*80)
  return

processa_prompt(modelo, "Once upon a time")


