# Exploring LLMs

Explorando as capacidades dos Modelos de Linguagem (LLMs) com a biblioteca Transformers da Hugging Face.

## üìö Checkpoint 04

**Autores:**  
- Jo√£o Victor Santos Alves (RM99634)  
- Luiza Barros Hubert (RM97905)  
- Suellen Guedes Rufino (RM97687)  

## üóÇÔ∏è Notebooks

1. [EntendendoLLMs.ipynb](./EntendendoLLMs.ipynb)
2. [Tokens,DevicesEMem√≥riaDeConversa.ipynb](./Tokens,DevicesEMem√≥riaDeConversa.ipynb)
3. [FiltrandoJailbreak.ipynb](./FiltrandoJailbreak.ipynb)

![Notebooks](https://tse3.mm.bing.net/th?id=OIP.r8ON7toWMgnFRjdRGLCMowHaC-&pid=Api&P=0&h=180) <!-- Substitua pelo caminho da sua imagem -->

---

## üìñ Descri√ß√£o do Projeto

Este projeto visa desenvolver um sistema de detec√ß√£o de prompts de jailbreak utilizando modelos de linguagem (LLMs) ajustados. O sistema combina dois modelos: um classificador de prompts e um gerador de texto.

### üîç O que √© Fine-Tuning?

**Fine-tuning** √© o processo de ajustar um modelo de aprendizado de m√°quina pr√©-treinado em uma nova tarefa, utilizando um conjunto de dados espec√≠fico. Isso permite que o modelo aprenda a realizar tarefas especializadas, melhorando seu desempenho em situa√ß√µes espec√≠ficas.

### üö® O que √© Jailbreak?

**Jailbreak** refere-se a t√©cnicas utilizadas para contornar as limita√ß√µes de seguran√ßa de um modelo de linguagem, possibilitando a gera√ß√£o de conte√∫dos indesejados ou maliciosos. Prompts de jailbreak s√£o comandos projetados para manipular o comportamento do modelo, levando-o a gerar respostas inadequadas.

## ‚öôÔ∏è Implementa√ß√£o

Neste notebook, utilizamos:

- **Classificador de Jailbreak:** dispon√≠vel em [Hugging Face](https://huggingface.co/jackhhao/jailbreak-classifier)
- **Modelo de Gera√ß√£o de Texto:** [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)

### üõ†Ô∏è Funcionalidade

A fun√ß√£o principal do sistema √© a seguinte:

```python
from transformers import pipeline

# Carregar o classificador de jailbreak
jailbreak_classifier = pipeline("text-classification", model="jackhhao/jailbreak-classifier")
# Carregar o modelo generativo
text_generator = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")

def process_prompt(user_prompt):
    # Checar se o prompt √© potencialmente malicioso
    classification = jailbreak_classifier(user_prompt)
    if classification[0]['label'] == 'jailbreak':
        return "Este prompt √© potencialmente malicioso e n√£o ser√° processado."
    else:
        # Gerar texto seguro
        response = text_generator(user_prompt, max_new_tokens=100)
        return response[0]['generated_text']
